---
output:
  pdf_document:
    latex_engine: xelatex
    highlight: pygments
mainfont: Calibri Light
sansfont: Calibri Light
fontsize: 11pt
geometry: margin=1in
---

**Homework 05**  
**Joseph Blubaugh**  
jblubau1@tamu.edu  
**STAT 641-720**  

\newpage

I.  This distribution would have most of mass centralized and looks symmetrical except for some extreme values on both the left and right side.

II.  
  1.

```{r q2, echo=FALSE, fig.height=3}

S = c(0.42, 0.86, 0.88, 1.11, 1.34, 1.38, 1.42, 1.47, 1.63,
      1.73, 2.17, 2.42, 2.48, 2.74, 2.74, 2.79, 2.90, 3.12,
      3.18, 3.27, 3.30, 3.61, 3.63, 4.13, 4.40, 5.00, 5.20,
      5.59, 7.04, 7.15, 7.25, 7.75, 8.00, 8.84, 9.30, 9.68,
      10.32, 10.41, 10.48, 11.29, 12.30, 12.53, 12.69, 14.14, 14.15,
      14.27, 14.56, 15.84, 18.55, 19.73, 20.00)

L = c(.94, 1.26, 1.44, 1.49, 1.63, 1.80, 2, 2, 2.56,
      2.58, 3.24, 3.39, 3.53, 3.77, 4.36, 4.41, 4.6, 4.67,
      5.39, 6.25, 7.02, 7.89, 7.97, 8, 8.28, 8.83, 8.91,
      8.96, 9.92, 11.36, 12.15, 14.4, 16, 18.61, 18.75,
      19.05, 21, 21.41, 23.27, 24.71, 25, 28.75, 30.23, 35.45,
      36.35) 

qqplot(x = S, y = L)
```

  2.

```{r q2b, echo=FALSE, fig.height=3}
par(mfrow=c(1,2))
qqnorm(S)
qqnorm(L)
```

  3. Larger litters tend to have larger brain weight relative to body weight. There is also larger variance in relative brain weight for larger litters.


III.

```{r q31, echo=FALSE}

par(mfrow = c(2, 3))

## exponential(3)
x1 = qexp(.25, rate = 3) - (1.5 * (qexp(.75, 3) - qexp(.25, 3)))
x2 = qexp(.75, rate = 3) + (1.5 * (qexp(.75, 3) - qexp(.25, 3)))
plot(x = seq(0, 2, .1), y = dexp(seq(0, 2, .1), 3), type = "l",
     xlab = "", ylab = "")
abline(v = 1.011, col = "blue")
title(main = paste("Exponential(3) \n",
                   "p(X < 0) = 0, \n",
                   "p(X > 1.0114) = ",
                   round(1-pexp(1.0114, 3), 4)))

## weibull(4, 3)
x1 = qweibull(.25, 4, 3) - (1.5 * (qweibull(.75, 4, 3) - qweibull(.25, 4, 3)))
x2 = qweibull(.75, 4, 3) + (1.5 * (qweibull(.75, 4, 3) - qweibull(.25, 4, 3)))
plot(x = seq(0,6,.1), dweibull(seq(0,6,.1), shape = 4, scale = 3), 
     type = "l", xlab = "", ylab = "")
abline(v = c(.609, 4.84), col = "blue")
title(main = paste("Weibull(4,3) \n", 
                   "p(X < .609) =", 
                   round(pweibull(.609, 4, 3), 4),
                   "\n p(X > 4.842) = ", 
                   round(1 - pweibull(4.84, 4, 3), 4)))


## uniform on (0,1)
x1 = qunif(.25, 0, 1) - (1.5 * (qunif(.75, 0, 1) - qunif(.25, 0, 1)))
x2 = qunif(.75, 0, 1) + (1.5 * (qunif(.75, 0, 1) - qunif(.25, 0, 1)))
plot(x = seq(0, 1, .1), y = dunif(seq(0, 1, .1), 0, 1), type = "l",
     xlab = "", ylab = "")
segments(y0 = 0, y1 = 1, x0 = c(0,1))
title(main = "Uniform(0,1) \n P(X < 0) = 0, P(X > 1) = 0")


## normal
x1 = qnorm(.25, 0, 1) - (1.5 * (qnorm(.75, 0, 1) - qnorm(.25, 0, 1)))
x2 = qnorm(.75, 0, 1) + (1.5 * (qnorm(.75, 0, 1) - qnorm(.25, 0, 1)))
plot(x = seq(-3, 3, .1), y = dnorm(seq(-3, 3, .1), 0, 1), type = "l",
     xlab = "", ylab = "")
abline(v = c(-2.697, 2.697), col = "blue")
title(main = paste("Normal(3,1) \n", 
                   "p(X < -2.697) =", 
                   round(pnorm(-2.697, 3, 1), 4),
                   ", \n p(X > 2.697) =",
                   round(1-pnorm(2.697, 3, 1), 4)))

## t with df=2
x1 = qt(.25, 2) - (1.5 * (qt(.75, 2) - qt(.25, 2)))
x2 = qt(.75, 2) + (1.5 * (qt(.75, 2) - qt(.25, 2)))
plot(x = seq(-3.5, 3.5, .1), y = dt(seq(-3.5, 3.5, .1), 2), type = "l",
     xlab = "", ylab = "")
abline(v = c(-3.265, 3.265), col = "blue")
title(main = paste("T(2) \n",
                   "p(X < -3.265) =",
                   round(pt(-3.265, 2), 4),
                   "\n p(X > 3.265) = ",
                   round(1 - pt(3.265, 2), 4)))



```

IV.  Since all $E_i$ must be larger than 1, defects 3 or more must all be combined. Also we need to use MLE as an estimator for the unknown probability of failure which in this case is .26. The Chi-Squared GOF results in a very low p-value so we can conclude that the model is a poor fit.

```{r q4, echo=FALSE}

MLE = ( (0*121) + (1*110) + (2*38) + (3*11)) / (280*3)

Oi= c(121, 110, 38, 11)
pi = round(dbinom(0:3, 3, prob = .26), 4)
Ei = round(pi * 280, 1)
Qi = round((Oi - Ei)^2 / Ei, 4)

(dt = data.frame(pi, Ei, Oi, Qi))

paste("Chi-Squared GOF:", round(1 - pchisq(sum(Qi), 3), 4))

```


V.  
1.  QQ Plots of CPUE and Log-Normal are similarly shaped. The goodness of fit test tells us that the model fits well when using a Log-Normal model with paramters specified by the MLE. 

```{r q5, echo=FALSE, fig.height=4}
library(MASS)

dt = c(0.6, 0.7, 1.1, 1.3, 1.8, 2.0, 2.3, 2.7, 2.9, 3.1,
       3.9, 4.3, 4.4, 4.9, 5.2, 5.4, 6.1, 6.8, 7.1, 8.0,
       9.4, 10.3, 12.9, 15.9, 16.0, 22.0, 22.2, 22.5, 23.0, 23.1,
       23.9, 26.5, 26.7, 28.4, 28.5, 32.2, 40.2, 42.5, 47.2, 48.3,
       55.8, 57.0, 57.2, 64.9, 67.6, 71.3, 79.5, 114.5, 128.6, 293.5)

## bins -> 0-9 (21), 10-29 (14), 30-69 (10), 70+ (5)

Oi = c(21, 14, 10, 5)

par(mfrow = c(1,2))
qqnorm(dt, main = "Normal Q-Q of CPUE")
qqnorm(dlnorm(x = seq(0, 10, .5)), main = "Normal Q-Q of LogNormal")

fitdistr(dt, densfun = "log-normal")
pi = c(round(plnorm(q = 9.999, meanlog = 2.5906275, sdlog = 1.4388286), 4),
       round(plnorm(q = 29.999, meanlog = 2.5906275, sdlog = 1.4388286), 4),
       round(plnorm(q = 69.999, meanlog = 2.5906275, sdlog = 1.4388286), 4),
       round(plnorm(q = 293.5, meanlog = 2.5906275, sdlog = 1.4388286), 4))
pi = diff(pi)
pi = c(.4179, pi)

Ei = round(pi * 50, 1)
Qi = round((Oi - Ei)^2 / Ei, 4)

(dt = data.frame(pi, Ei, Oi, Qi))
paste("Log-Normal GOF:", round(1 - pchisq(sum(Qi), 3), 3))

```

2.  The best estimate for $\theta$ is .073. Both $\theta = log(x)$ and $\theta = x^{.073}$ pass the shapiro test for normality, and $\theta = .073$ performs better, but log is probabily the better choice because its easier to interpret.

```{r q52, echo=FALSE}
x = c(0.6, 0.7, 1.1, 1.3, 1.8, 2.0, 2.3, 2.7, 2.9, 3.1,
       3.9, 4.3, 4.4, 4.9, 5.2, 5.4, 6.1, 6.8, 7.1, 8.0,
       9.4, 10.3, 12.9, 15.9, 16.0, 22.0, 22.2, 22.5, 23.0, 23.1,
       23.9, 26.5, 26.7, 28.4, 28.5, 32.2, 40.2, 42.5, 47.2, 48.3,
       55.8, 57.0, 57.2, 64.9, 67.6, 71.3, 79.5, 114.5, 128.6, 293.5)

x1= x^.073
x2 = log(x)

shapiro.test(x1)
shapiro.test(x2)

```



VI.  
Plot1:  Mixture of .90 Normal(10, 1) & .010 Normal(30, 3)  
Plot2:  Uniform(0, .7)  
Plot3:  Gamma(1.2, 25)  
Plot4:  Exponential(80)  


VII.  
1.  (E): AD does a better job a detecting departures in the tails of the distribution  
2.  (A): AD is best because it looks at the differences in the entire model and has greater sensitivity  
3.  (D): AD Distribution free models are completely specified so their parameters do not have to be estimated  
4.  (E): Chi-Square test requires transforming continuous distributions into discrete distributions and has the some problem as relative frequency histograms  
5.  (C): Dealing with count data in a distrete distribution so GOF would work  
6.  (D): 